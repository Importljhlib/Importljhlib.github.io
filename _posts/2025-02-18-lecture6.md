---
layout: post
title: cs231n - lecture 6 정리
date: 2025-02-18 00:00:00 + 0000
category: cs231n
---
# lecture 6

### Training Neural Networks

먼저 Activation Functions에 대해서 알아보자.

![](/img/cs231n6-0.png)

시그모이드의 문제점)

1. x가 큰 음수이거나 큰 양수일 때 flat하기 때문에 기울기가 거의 0이 되어 소실된다,
2. 시그모이드의 출력이 zero centered하지 않다. 만약 x가 항상 양수라면 w의 gradient는 시그모이드 upstream gradient와 부호가 항상 같게 된다. 이는 W가 모두 양의 방향이나 모두 음의 방향으로만 증감하는 zig zag path를 따르게 되기 때문에 비효율적이다. 

![](/img/cs231n6-1.png)

두번째로 알아볼 활성화 함수는 tanh이다.

![](/img/cs231n6-2.png)

sigmoid랑 유사하지만 범위가 [-1, 1]이고, zero centered라는 것이다. 하지만 여전히 flat해지는 구간이 나타난다.

세번째는 ReLU 함수이다.

![](/img/cs231n6-3.png)

ReLU 함수는 앞의 두 함수와는 다르게 양의 값에서는 saturetion되지 않는다. (flat하지 않기 때문.) 또한 수렴 속도가 빠르다.

하지만 ReLU는 zero-centered가 아니라는 점과 음의 경우에서는 saturetion된다는 문제점이 있다.

![](/img/cs231n6-4.png)

초기화를 잘못하거나 지나치게 learning rate를 사용한 경우 dead ReLU가 발생할 수 있다. 

그래서 ReLU를 초기화할 때 positive biases를 추가하는 경우도 있다.

![](/img/cs231n6-5.png)

다음은 Leaky ReLU 함수이다. ReLU와 유사하지만 음수 영역에서 더이상 0이 아닌 것을 볼 수 있다. 따라서 음수 영역에서도 saturation이 되지 않는다. dead ReLU로 발생 x

![](/img/cs231n6-6.png)

이 함수는 PReLU이고, 기울기 alpha로 결정된다.

![](/img/cs231n6-7.png)

출력이 zero mean에 가깝다. 또한 음에서 saturation이지만, noise에 강인하다.

![](/img/cs231n6-8.png)

보통 ReLU를 많이 사용한다. 그리고 sigmoid는 절대 사용하면 안된다.

### Data Preprocessing

가장 기본적인 데이터 전처리 과정은 zero-mean으로 만들고 normalize하는 것이다.

각각의 데이터에 평균값을 빼고, 표준편차로 나눠주어서 스케일을 맞추는 것.

![](/img/cs231n6-9.png)

normalization을 해주는 이유는 모든 차원이 동일한 범위안에 있게 해줘서 전부 동등한 기여를 하게 해준다. 이미지는 보통 스케일리 맞춰져 있어서 zero mean만 해준다.

### Weight Initialization

![](/img/cs231n6-10.png)

만약 모든 가중치가 0 이면 어떻게 될까? 

- 모든 뉴런이 같은 일을 할 것이다. 모든 연산이 같을 것이고, 출력도 같고 결국 gradient도 서로 같을 것이다.

Q. gradient는 가중치 뿐만 아니라 loss의 영향도 받으므로 결국 각각의 gradient는 다르지 않나?

A.  각 뉴런이 어떤 클래스에 연결되어 있는지에 따라 서로 다른 loss를 가질 수는 있지만, 네트워크 전체를 보면 많은 뉴런들이 동일한 가중치로 연결되어 있기 때문에 문제가 생길 것이다.

초기화 문제를 해결하는 첫번째 방법은 임의의 작은 값으로 초기화하는 것이다.

![](/img/cs231n6-11.png)

초기 W를 표준정규분포에서 샘플링한다.

하지만 딥뉴럴넷에서는 문제가 발생할 수 있다.

![](/img/cs231n6-12.png)

10개의 레이어와 레이어당 500개의 뉴런으로 이루어진 네트워크에 tanh Activation을 사용한 결과이다.

출력값이 0에 점점 가까워지는 것을 볼 수 있다. Backwork pass에서도 upstream gradient를 구하는 것은 현재 upstream에 가중치를 곱하는 것이기 때문에 점점 gradient가 작아져 0에 수렴할 것이다.

그렇다면 가중치를 큰 값으로 가져가면 어떻게 될까?

값들이 saturation될 것이다.

이들을 보완하는 방법이 바로 Xavier initialization이다.

![](/img/cs231n6-13.png)

standard gaussian으로 뽑은 값을 입력의 수로 스케일링 해준다.

기본적으로 Xavier init이 하는 일은 입/출력의 분산을 맞춰주는 것이다. 입력의 수가 작으면 더 작은 값으로 나눠서 좀 더 큰 값을 얻는다. 

더 큰 가중치가 필요한데, 작은 입력의 수가 가중치와 곱해지기 때문에 가중치가 더 커야만 출력의 분산만큼 큰 값을 얻을 수 있기 때문이다. 

Xavier initialization은 더 찾아봐야 할 것 같다.

가중치 초기화는 모델의 학습 속도와 성능에 큰 영향을 미칩니다. Xavier 초기화는 입력과 출력의 개수를 고려하여 가중치를 초기화하고, He 초기화는 ReLU 활성화 함수와 함께 사용될 때 더 효과적입니다.

다음 방법은 Batch Normalization이다.

![](/img/cs231n6-14.png)

우리는 레이어의 출력이 unit gaussian이길 바라는데, Batch norm은 강제로 그렇게 만드는 것이다. 

현재 batch에서 계산한 mean과 variance를 이용해서 normalization을 할 수 있다.

따라서 가중치를 잘 초기화 시키는 것 대신에 학습할 때마다 각 레이어가 모두 unit gaussian이 되도록 해주는 것이다.

![](/img/cs231n6-15.png)

그리고 이 연산은 FC나 Conv layer 직후에 넣어준다.

![](/img/cs231n6-16.png)

여기서 한가지 문제점이 있는데, 우리는 unit gaussian이 되어 항상 saturation이 일어나지 않는 것을 바라는게 아니다. saturation을 조절하는 것이 목표이기 때문에 scaling연산을 추가한다.

![](/img/cs231n6-17.png)

### Babysitting the Learning Process

1. 데이터 전처리
2. 아키텍쳐 선택
3. 초기 loss 체크
    1. regulaization term 추가 후 loss 체크
4. 일부 데이터 학습
    1. regularization을 사용하지 않고 epoch마다 loss가 내려가는지 확인, Train Accuracy 증가 확인
5. 전체 데이터 사용
    1. regularization을 약간만 주면서 적절한 learning rate 찾기
    2. learing rate가 작으면 gradient 업데이트가 충분히 일어나지 않아 loss가 잘 줄어들지 않는다. 
    3. 반대로 너무 크면 NaNs 값이 된다.
    4. 보통 e^-3 ~ e^-5를 쓴다.

### Hyperparameter Optimization

하이퍼 파라미터를 최적화하는 전략중 하나는 cross validation이다. 

training set으로 학습, validation set으로 평가.

1. coarse stage에서 epoch 몇 번으로 현재 값이 잘 동작하는지 판단한 후 범위를 결정한다.
2. fine stage에서는 좀 더 좁은 범위를 설정하고 학습을 더 길게 시켜보면서 최적의 값을 찾는다. train 동안 cost가 이전의 3배 높아지거나 하면 NaNs가 나오니, 빠르게 다른 하이퍼 파라미터를 선택하자.

loss 커브를 보고,

loss가 발산하면 learing rate가 높은 것. 너무 평평하다면 너무 낮은 것.

loss가 평평하다가 갑자기 빠르게 가파르게 내려가면 초기화 문제이다. gradient의 backprop이 초기에는 잘 되지 않다가 학습이 진행되면서 회복되는 경우.

train_acc와 va_acc가 큰 차이를 보인다면 오버핏일수도 있으니 regularization의 강도를 높이자. 차이가 없다면 아직 오버핏하지 않은 것이고, capacity를 높힐 수 있는 여유가 있다는 것을 의미한다.

![](/img/cs231n6-18.png)

가중치의 크기 대비 가중치 업데이트의 비율은 0.001이 좋음.