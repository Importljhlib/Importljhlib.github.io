---
layout: post
title: cs231n - lecture 5 정리
date: 2025-02-17 01:54:00 + 0900
category: cs231n
---
# lecture 5

### Convolutional Neural Networks

먼저 이전 강의에서 배웠던 Fully Connected Layer에 대해서 알아보자.

![](/img/images_cha-suyeon_post_33876655-d5c8-438f-961f-724402f0e62d_image.png)

FC Layer에서 하는 일은 어떤 벡터를 가지고 연산을 하는 것이다. 

32x32x3 이미지를 3072 차원 벡터로 핀 다음에 가중치 W와 곱해서 (1, 10)의 activation이 만들어 진다.

Convolution Layer와 FC Layer의 주된 차이점은 Convolution Layer는 기존의 구조를 보존시킨다는 것이다.

![](/img/images_cha-suyeon_post_73cb7fc9-ae8d-4e24-ba9c-0044fe7460a7_image.png)

그리고 저 작은 필터가 우리가 가진 가중치가 되는 것이고, 이 필터를 가지고 이미지를 슬라이딩하면서 공간적으로 내적을 수행하게 된다.

그리고 input의 depth와 필터의 depth와 같다.  

![](/img/images_cha-suyeon_post_4d74f760-0113-4718-9ee9-fc5aca9c1204_image.png)

필터가 어떻게 슬라이딩하면서 output를 뽑아내냐면, 겹쳐놓고 내적, 슬라이딩해서 옆에서 계속 내적해서 output activation map의 해당 위치에 전달하는 것이다.

![](/img/cs231n5-0.png)

![](/img/cs231n5-1.png)

사진처럼 한 Layer에서 원하는 만큼 여러개의 필터를 사용할 수 있다. 필터마다 다른 특징을 추출할 수 있는 것이다.

![](/img/cs231n5-2.png)

각각의 사이에는 activation과 pooling이 들어간다. layer는 여러개의 필터를 가지고 있고, 각 필터마다 각각의 출력 map을 만들게 된다. 여러 레이어들을 거치면서 각 필터들이 계층적으로 학습이 가능해지는 것. 참고로 위 사진에서 필터의 depth를 맞추고 있는 것을 볼 수 있다.

![](/img/cs231n5-3.png)

CNN의 전체적인 과정이다. Conv, ReLU, Conv, ReLU같이 반복적으로 거치고 나면 pooling layer를 거치게 된다. pooling layer는 activaition maps의 사이즈를 줄이는 역할을 한다. 또한 끝에는 FC-Layer를 통해 최종 스코어를 계산하게 된다.

위에서 알아봤던 32x32x3 이미지가 5x5x3 필터를 가지고 연산을 통하여 28x28 activation map이 생기는 과정을 자세히 알아보자.

![](/img/cs231n5-4.png)

만약 7x7 입력과 3x3의 필터가 있는데, 이 필터가 왼쪽 상단부터 시작해서 오른쪽 끝까지 한칸씩 슬라이딩 한다면 좌우 방향은 물론 상하 방향으로 5번 수행이 가능할 것이다. 그리고 슬라이딩하는 간격을 stride라고 하는데, stride가 2라면 3번씩 가능하게 될 것이다. 

stride가 3이면 이미지와 fit하지않기 때문에 사용X

![](/img/cs231n5-5.png)

 따라서 output size는 (N-F)/stride + 1로 정의할 수 있다.

![](/img/cs231n5-6.png)

위에서 이미지와 필터가 맞지 않는 상황을 방지하기, 입력 사이즈와 출력 사이즈를 같도록 하기 위해서 zero-pad라는 기법을 사용한다.

둘레에 0을 추가하는 것이다. 그렇다면 위 사진의 상황에선 (9-3)/1+1 = 7, 즉 7x7 output이 나오게 된다. (정확한 출력은 7 x 7 x ”필터의 개수”가 된다.)

따라서 Padding을 하게되면 출력의 사이즈를 유지시켜주고, 필터의 중앙이 닿지 않는 곳도 연산할 수 있게 된다.

1. input이 32x32x3, 10개의 5x5 필터, stride가 1, pad가 2일 때 output 사이즈는?

→ (32+2*2-5)/1+1 = 32, 따라서 32x32x10이 된다. (pad가 2면 양쪽에 2씩 추가, 즉 4만큼 늘어난다.)

1. 그렇다면 이 레이어의 파라미터는 몇개일까?

→ 각 필터는 5x5x3 + 1 = 76 params를 가지고 있다. (+1 for bias) 따라서 76*10 = 760개.

![](/img/cs231n5-7.jpg)

### Pooling layer

![](/img/cs231n5-8.jpg)

이제 pooling에 대해서 살펴보자.

Pooling layer가 하는 일은 간단하게 activation map을 downsample을 하는 것이다.

224x224x64인 입력이 있다면 이를 112x112x64로 공간적으로 줄여준다.

여기서 중요한 점은 Depth에는 영향을 주지 않는다는 것이다. 

![](/img/cs231n5-9.png)

일반적으로 Max pooling을 사용한다. pooling에도 필터 크기를 정할 수 있는데, 얼마만큼의 영역을 한 번에 묶을지 정한다. max pooling은 Conv layer 처럼 슬라이딩하면서 내적이 아닌 가장 큰 값을 고르게 된다.

padding은 크기 보존 + 이미지의 부분적 특징을 살리기 위한 것.

pooling은 이미지의 특정 부분을 잘라내는 것이 아닌, 사진 전체 부분을 유지한 상태로 픽셀만 줄이는 것. 이라고 보면 된다.

![](/img/cs231n5-10.png)