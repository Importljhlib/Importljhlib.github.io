---
layout: post
title: VGGNet - Very Deep Convolutional Networks For Large-Scale Image Recognition
date: 2025-02-21 01:53:00 + 0900
category: paper
---
# VGGNet : Very Deep Convolutional Networks For Large-Scale Image Recognition

## Abstract

네트워크의 깊이가 모델의 좋은 성능을 보였다.

특히 아주 작은 3x3 필터를 사용함으로써 모델의 깊이를 16 ~ 19 layer 까지 늘릴 수 있었다는 점이 중요하다.

## Introduction

마찬가지로 작은 3x3 convolution 필터를 적용함으로써 뛰어난 성능을 발휘할 수 있었다.

## ConvNet Configurations

![](/img/AlexNet-and-VGGNet-architecture.png)

VGG는 AlexNet보다 2배 이상 깊은 네트워크 학습에 성공.

오차율 절반 줄였다.

16~19레이어에 해당하는 깊은 신경망을 학습할 수 있었던 이유는 모든 합성곱 레이어에서 3x3 필터를 사용했기 때문이다.

### Architecture

![](/img/vgg16-neural-network.webp)

입력

- 크기 : 224 x 224의 RGB 이미지
- 전처리 : Train set의 픽셀값에서 평균 RGB 값을 빼준다.

Features

모든 Conv layer는 3x3, stride = 1, padding = 1을 사용하여 출력의 크기가 유지되다가 pooling에서 절반으로 줄어드는 구조를 가지고 있다.

Input (224x224x3)
↓
[Conv3-64] * 2 → MaxPool (112x112x64)
↓
[Conv3-128] * 2 → MaxPool (56x56x128)
↓
[Conv3-256] * 3 → MaxPool (28x28x256)
↓
[Conv3-512] * 3 → MaxPool (14x14x512)
↓
[Conv3-512] * 3 → MaxPool (7x7x512)
↓
Flatten (25,088)
↓
FC-4096 → Dropout
↓
FC-4096 → Dropout
↓
FC-1000 → Softmax

### Configuration

![](/img/vggimage1.jpg)

11 depth인 A구조부터 19depth인 E구조까지 있다고 한다.

depth가 늘어남에도 더 큰 conv layer를 사용한 얕은 신경망보다 오히려 파라미터 수가 줄어 들었다.

### Discussion

그렇다면 왜 더 큰 사이즈의 필터 하나를 사용하는 것이 아닌, 3x3 필터를 여러 개 사용하는 것일까.

3x3 필터를 2개 사용하는 것은 5x5 필터를 하나 사용하는 것과 동일한데, 이때 작은 상사이즈로 여러 번 나누어 적용하면 conv layer를 거칠 때 마다 ReLU도 더 많이 통과하게 되어 non-linear한 의사 결정을 더 잘하게 된다.

또한 3x3 3개가 7x7 1개보다 파라미터 수가 줄어드는데, 이는 7x7에 일반화를 적용한 것과 같아 오버피팅을 줄이는 효과가 있다고 한다.

그림을 통해서 자세히 알아보자.

Conv를 두 번 통과 시킨다고 생각해보자.

![](/img/vggimage2.png)

먼저 첫 번째로 통과 시킬 때를 보자.

하나의 픽셀을 만들어 내기 위해서 3x3 만큼의 픽셀들을 보고 만들어 질 것이다. 즉, receptive field가 3x3이 되는 것이다.

그렇다면 여기서 conv를 한 번 더 하면 어떻게 될까?

![](/img/vggimage3.png)

처음과 마찬가지로 가운데에 위치한 하나의 픽셀을 만들어 내기 위해서 3x3 만큼 보게 될 것이다. 그렇다면 중앙 말고 오른쪽 위의 픽셀을 볼 때는 어떻게 될까?

![](/img/vggimage4.png)

빨간색 위치의 픽셀들을 보고 만들어 내게 될 것이다.

최종적으로 가장 처음의 검은색 픽셀이 가지는 receptive field는 5x5가 될 것이다.

이것을 한 번 더 하면 7x7만큼 보게 된다. 계속 반복하면 더 넓은 영역을 볼 수 있게 되는 것이다.

## Classification Framework

### Trainning

loss : cross entropy

mini batch : 256

optimizer : momentum - 0.9

reg : L2 - 5.10^-4, Dropout - 0.5

Learning rate : 10^-2에서 val error rate가 높아질 수록 10^-1씩 감소

**가중치 초기화**

A모델을 학습한 뒤, 다음 모델을 구성할 때 A모델의 처음 4개 Conv layer와 마지막 3개의 Fc layer를 사용하여 최적의 초기값을 설정한다.

**학습 이미지 크기**

S (training image의 Scale 파라미터)가 224인 경우, training image의 가로, 세로 중 더 작은 쪽은 224에 맞춘다. 이렇게 rescale한 이미지에서 random하게 224 x 224로 crop하여 input size를 맞춘다.

1. Single-scale training

S를 256 또는 384로 고정 시키는 방법. 

2. Multi-scale training

S를 고정시키지 않고 256 ~ 512 값 중 random하게 값을 설정하는 방법.

이미지가 모두 같은 사이즈가 아니기 때문에 Multi-scale로 학습을 하면 학습 효과가 좋아진다. 

또한 Multi-scale로 학습을 할 때 S = 384로 미리 학습한 후 fintuning을 한다.

### Testing

Test를 할 때도 input을 rescale 해준다. Q (테스트 이미지의 Scale 파라미터)가 S와 같을 필요는 없다. 또한 각각의 S 값 마다 다른 Q를 적용할 때 VGG 모델의 성능이 좋아진다. (Val set을 이용해 평가와 동시에 학습을 한다는 의미?)

Validation을 수행할 때는 FC layer를 Conv layer로 바꿔준다.

첫 번째 FC layer를 7x7 conv layer로 바꾸고, 나머지 두 FC를 1x1로 바꾼다.

또한 레이어가 모두 Conv이기 때문에 Test 시에는 이미지 크기에 제약이 없어진다.

## Classification experiments

ILSVRC를 dataset으로 사용하였고, top-1과 top-5 error 방식을 사용하였다.

Evaluation는 Single-scale과 Multiscale 두 개로 나뉜다.

또한 validation set을 test set으로 사용하였다.

### Single Scale evaluation

![](/img/vggimage5.png)

Single Scale Training는 S = Q 사이즈로 test image의 사이즈가 고정된다.

또한 AlexNet에서 사용되었던 LRN이 효과가 없었던 것을 볼 수 있다.

depth가 깊어질 수록 error가 감소하는 것을 볼 수 있다.

### Multi Scale evaluation

![](/img/vggimage6.png)

Multi-scale evaluation은 test 이미지를 multi scale로 설정하는 방식인데, 

S가 고정이라면 Q = { S - 32, S, S + 32 }이고, 변화한다면 Q = { S_min, 0.5(S_min + S_max ), S_max)로 평가한다.

### Multi crop evaluation

![](/img/vggimage7.png)

multi-crop & dense, multi-crop, dense 순으로 좋은 성능을 보였다.

### Conclusion

네트워크의 깊이를 늘려도 좋은 성능을 보여줄 수 있다는 것을 보였다.