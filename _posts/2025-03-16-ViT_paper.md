---
layout: post
title: "An Image is Worth 16x16 Words: Transformers for Image Recognition At Scale"
date: 2025-03-16 01:53:00 + 0900
category: paper
---
## Introduction

컴퓨터 비전 분야에서는 CNN이 오랫동안 지배적인 모델 구조로 자리 잡고 있었으나, 자연어 처리에서 Transformer 기반 모델들이 큰 성과를 거두면서 이를 이미지 처리에 적용하는 연구가 활발해졌다.

이 논문에서는 **Vision Transformer(ViT)**라는 새로운 모델을 제안하며, 기존 CNN 기반 모델과의 성능 비교를 진행하였다. ViT는 Transformer 아키텍쳐를 기반으로 이미지를 학습하여 대규모 데이터셋에서 SOTA(State-of-the-Art) 성능을 기록할 수 있었다.

### Inductive Bias

Inductive bias는 training에서 보지 못한 데이터에 대해서도 적절한 귀납적 추론이 가능하도록 하기 위해 모델이 가지고 있는 가정들의 집합을 의미한다.

DNN의 기본적인 요소들의 inductive bias

- Fully connected : 입력 및 출력 element가 모두 연결되어 있으므로 구조적으로 특별한 relational inductive bias를 가정하지 않음.
- Convilutional : CNN은 작은 크기의 kernel로 이미지를 지역적으로 보며, 동일한 kernel로 이미지 전체를 본다는 점에서 locality와 transitional invariance 특성을 가진다.
- Recurrent : RNN은 입력한 데이터들이 시간적 특성을 가지고 있다고 가정하므로 sequentiality와 temporal invariance 특성을 가진다.

**Transformer는 CNN, RNN보다 상대적으로 inductive bias가 낮다.**

## Vision Transformer

![](/img/ViT_1.png)

### Image Patch Embedding

![](/img/ViT_2.png)

ViT에서는 image patches를 sequence로 바라본다. 

이미지를 작은 패치로 분할한 후, 각 패치를 token처럼 취급하게 된다.

H x W x C 크기의 이미지 x를 N x (P^2 * C) 크기의 flattened 2D patches x_p로 reshape 해준다.

(H, W) : 원본 이미지의 해상도

C : 채널 개수

(P, P) : 각 image patch의 해상도

N : reshape 결과가 나오게 되는 image patches의 개수

그리고 P^2*C 차원의 이미지 패치를 D차원으로 매핑시키는 linear projection을 해주게 된다.

![](/img/ViT_11.png)

### Position Embedding

![](/img/ViT_3.png)

Position embedding은 patch embedding과 더해져 모델의 input으로 들어가게 된다.

Transfomer는 입력 순서 정보를 명시적으로 알지 못하기 때문에 위치 정보를 추가적으로 제공하는 position embedding을 사용하는 것이다.

또한 2D Position embedding의 성능 향상이 보이지 않아서 1D position을 사용했다고 한다.

### Transformer Encoder

![](/img/ViT_4.png)

기존의 Transformer 구조와 비슷하다.

Multi-head Self Attention과 MLP block으로 구성되어 있다.

MLP는 2개의 layer를 가지고 있고, GELU 활성화 함수를 사용한다.

기존과 마찬가지로 각 block 앞에는 Layer Norm, 각 block 뒤에는 residual connection을 적용하였다.

![](/img/ViT_5.png)

### inductive bias

## Experiments

### Datasets

ViT는 class와 이미지의 개수가 각각 다른 3개의 데이터셋을 기반으로 pre-train 되었다.

- ImageNet - 1k (1.3M)
- ImageNet - 21k (14M)
- JFT - 18k (303M)

다음의 벤치마크 tasks를 downstream task로 하여 pre-trained ViT의 representation 성능을 검증하였다.

- ReaL lables : CIFAR-10/100, Oxford-IIIT Pets, Oxford Flowers-102
- 19-task VTAB classification suite

### Model Variants

ViT는 총 3개의 volume에 대해서 실험을 진행하였다.

![](/img/ViT_6.png)

### Comparsion to SOTA

![](/img/ViT_7.png)

실험의 결과는 위와 같다.

14x14 패치 크기를 사용한 ViT-Huge와 16x16 패치 크기를 사용한 ViT-Large의 성능을  비교하였다.

JFL 데이터셋에서 pre-training한 ViT-L/16 모델이 모든 downstream task에 대하여 보다 높은 성능을 도출해낸 것을 볼 수 있다.

또한 14x14 패치를 사용한 모델이 더 뛰어난 성능을 도출한 모습도 볼 수 있다.

![](/img/ViT_8.png)

또한 위 사진은 VTAB dataset에 대한 성능 비교이다. 

### Pre-training Data Requirements

다음 실험은 pre-training 데이터셋의 크기에 따른 fine-tuning 성능을 확인한 것이다.

![](/img/ViT_9.png)

각 데이터셋에 대하여 pre-training한 ViT를 ImageNet에 transfer learning한 결과 데이터가 클수록 ViT가 BiT 보다 성능이 좋고 크기가 큰 ViT 모델이 효과가 있었음을 알 수 있다.

또한 JFT를 각각 다른 크기로 랜덤 샘플링한 데이터셋을 활용하여 실험을 진행하였을 때, 작은 데이터셋에서는 CNN의 inductive bias가 효과가 있으나 큰 데이터셋에서는 데이터로부터 패턴을 학습하는 것만으로 충분함을 알 수 있다.

즉, ViT는 대량의 데이터를 학습할 수 있는 환경에서 강력한 성능을 발휘한다.

### Inspecting Vision Transformer

ViT가 어떻게 이미지를 처리하는지 이해하기 위한 실험.

![](/img/ViT_10.png)

Left : flatten 패치를 임베딩으로 변환하는 linear projection의 principal components를 분석하였다.

Center : 패치 간 position 임베딩의 유사도를 통해 가까운 위치에 있는 패치들의 position 임베딩이 유사한지 확인하였다.

Right : ViT의 레이어별 평균 attention distance를 확인한 결과, 초반 레이어에서도 attention을 통해 이미지 전체의 정보를 통합하여 사용함을 알 수 있다.