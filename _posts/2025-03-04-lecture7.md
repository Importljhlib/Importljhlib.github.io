---
layout: post
title: cs231n - lecture 7 정리
date: 2025-03-04 00:00:00 + 0000
category: cs231n
---
# lecture 7

### Training Neural Networks

가장 먼저 Optimization에 대해서 알아보자.

우리가 앞에서 배웠던 SGD와 같은 간단한 알고리즘은 몇가지 문제점이 있다.

![](/img/cs7_1.png)

위 그림을 보면 알 수 있듯이, Loss가 한 방향으로는 엄청나게 민감한 반면에 다른 방향으로는 덜 민감한 것을 볼 수 있다. 이런 상황에서 SGD는 지그재그 형태의 학습 과정을 볼 수 있다. 왜냐하면 이런 함수에서는 gradient의 방향이 고르지 못하기 때문이다. 이 예시는 2차원에 불과하기에 만약 고차원의 경우에는 수억개의 방향으로 움직일 수 있고, 이중에 불균형한 방향이 존재한다면 SGD는 잘 동작하지 않을 것이다.

![](/img/cs7_2.png)

다음 문제는 local minima와 saddle point 문제가 있다. 

global minima에 도달하지 못하고 gradient = 0이 되는 지점에 도착할 수도 있게 된다.

또한 saddle point 근처에서도 업데이트가 굉장히 느리게 되는 문제점도 발생한다.

이를 해결하는 방법은 SGD에 Momentum term을 추가하는 것이다.

![](/img/cs7_3.png)

아이디어는 그저 velocity를 유지하는 것이다. gradient를 계산할 때 velocity를 이용한다.

현재 미니배치의 gradient 방향만 고려하는 것이 아닌, velocity를 같이 고려하는 것.

여기에는 하이퍼 파라미터 rho가 추가되었는데, velocity의 영향력을 rho의 비율로 맞춰준다.

 

![](/img/cs7_4.png)

공이 떨어질 때 가속도를 받으면 gradient가 0이라도 그 구간을 빠져나올 수 있다.

또한 지그재그로 움직이는 상황일 때 momentum이 이 변동을 서로 상쇄시켜버린다.

![](/img/cs7_5.png)

Momentum의 변형인 Nesterov Momentum도 있다.

기본 SGD momentum은 현재 지점에서 gradient를 계산한 뒤 velocity와 섞어주지만,

Nesterov는 velocity 방향으로 움직인 뒤에 그 지점에서의 gradient를 계산한다. 그리고 다시 원점으로 돌아가서 둘을 합치는 것이다.

velocity의 방향이 잘못되었을 때 gradient로 보정해준다고 생각하면 된다.

![](/img/cs7_6.png)

![](/img/cs7_7.png)

+ velocity의 초기값은 0, 평평한 minima가 더 robust하다. 

Nesterov가 일반 momentum에 비해서 overshooting이 덜하다.

![](/img/cs7_8.png)

AdaGrad는 훈련도중 계산되는 gradients를 활용하는 방법이다.

AdaGrad는 velocity term 대신에 grad squared term을 이용한다. 학습도중에 계산되는 gradient에 제곱을 해서 더해주는 것. 그리고 update할 때 update term을 앞서 계산한 gradienet 제곱 항으로 나눠준다. 수식에 1e-7을 더하는 이유는 grad squared 값이 0인 경우를 없애기 위해서이다.

small dimension에서는 제곱합이 작으니깐 가속이 붙지만, large dimension에서는 속도가 줄어든다.

진행할수록 step이 작아지고, convex에선 좋지만 이 외의 경우에는 saddle에 걸리면 멈춘다는 단점이 있다.

![](/img/cs7_9.png)

이를 개선한 것이 RMSProp이다. 

gradient 제곱 항을 그대로 사용하지만, 이 값들을 그냥 누적 시키는 것이 아닌 decay rate를 곱해준다. decay rate는 보통 0.9 또는 0.99를 사용한다. 그리고 현재 gradient의 제곱은 (1 - decay rate)를 곱해줘서 더한다.

![](/img/cs7_10.png)

momentum 계열과 ada 계열을 합친 방법이 바로 Adam이다.

![](/img/cs7_11.png)

이 방법은 거의? Adam이다. 

first_moment은 Momentum , secend momentum은 제곱항이다. 마치 RMSProp + momentum 같아 보인다.

하지만 이러한 방법은 문제점이 있다.

초기 step에서, secend_moment의 값이 굉장히 작은 값일 수 있는데 이는 분모의 값이기 때문에 update step에서 값이 튀어서 이상한 곳으로 갈 수도 있을 것이다.

이를 해결하기 위해 bias correction term을 추가한다.

![](/img/cs7_12.png)

Adam은 beta1 = 0.9, beta2 = 0.999, 학습률 e-3, e-4 정도만 설정해줘도 거의 모든 아키텍쳐에서 잘 작동한다고 한다.

![](/img/cs7_13.png)

우리는 또한 learning rate decay라는 방법도 사용할 수 있다.

![](/img/cs7_14.png)

학습이 진행됨에 따라서 learning rate를 줄이는 전략이다.

![](/img/cs7_15.png)

loss가 완만해지면 learning rate를 줄인다!

![](/img/cs7_16.png)

![](/img/cs7_17.png)

우리는 지금까지 1차 근사함수를 실제 손실함수라고 가정하고 Step을 내려왔다. 하지만 위의 그림과 같이 멀리 갈 수 없다는 것을 알 수 있다.

따라서 우리는 2차 근사의 정보를 추가적으로 활용할 수 있다. 

2차 근사는 특정 지점에서 기존함수에 근사하는 2차함수를 찾는 것이다. 원래는 어떤 점에서의 기울기를 갖는 1차 함수 직선을 구했다면, 2차 근사에선 그 기울기를 갖는 2차 함수를 구하는 것. 

![](/img/cs7_18.png)

Hessian matrix의 역행렬을 이용하게 되면 실제 손실함수의 2차 근사를 이용해 minima로 곧장 이동할 수 있다. 이 알고리즘은 learning rate가 없다는 것을 볼 수 있다.

하지만 Hessian matrix는 NxN 행렬, N은 Network의 파라미터 수이기 때문에 deep learning에서는 사용할 수 없다.

그래서 실제로는 quasi-Newton methods를 이용한다.

![](/img/cs7_19.png)

Model Ensembles

![](/img/cs7_20.png)

여러 독립적인 모델들을 학습시키고, 결과의 평균을 사용하는 것이다.

약 2%정도의 성능 보정이 이루어질 수 있다. 

Q. 앙상블 모델마다 하이퍼 파라미터를 동일하게 줘야하나?

A. 모델 사이즈, LR, regularization 등을 다양하게 앙상블 할 수 있다.

이제는 Regularization에 대해서 알아보자.

이미 몇가지 regularization 기법에 대해서 살펴봤는데, Loss에 추가적인 항을 삽입하는 방법이었다. 

하지만 L1, L2는 NN에서는 잘 사용하지 않는다고 한다. 대신에 Batch Normalization(BN)과 Dropout을 사용한다고 한다.

![](/img/cs7_21.png)

Dropout은 아주 간단한데, forward pass할 때마다 일부 뉴런을 0으로 만드는 것이다.

forward pass iteration마다 모양이 계속 바뀌는 것. (activations를 0으로 만드는 것이다.)

```python
p = 0.5 # probability of keeing a unit active. higher = less dropout

def train_step(x):
	""" X contains the data """
	
	# forward pass for example 3-layer neural network
	H1 = np.maximum(0, np.dot(W, X) + b1)
	U1 = np.random.rand(*h1.shape) < p  # first dropout mask
	H1 *= U1 #drop!
	H2 = np.maximum(0, np.dot(W2, H1) + b2)
	U2 = np,random.rand(*H2.shape) < p # second dropout mask
	H2 *= U2 # drop!
	out = np.dot(W3, H2) + b3
```

![](/img/cs7_22.png)

Dropout을 하는 이유는 특징들 간의 상호작용(co-adaptation)을 방지하기 위해서이다.

네트워크가 일부 feature에 의존하는 현상을 막아주는 것. 결과적으로 overfitting을 막아준다.

또한 단일 모델로 앙상블 효과를 가질 수 있다는 관점도 있다.

forward pass마다 랜덤으로 dropout하기 때문에 forward마다 다른 모델을 만드는 것과 같은 효과.