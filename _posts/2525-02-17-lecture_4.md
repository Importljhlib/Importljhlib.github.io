---
layout: post
title: cs231n - lecture 4
date: 2025-02-17 01:53:00 + 0900
category: cs231n
---
# lecture 4

### Backpropagation

![](/img/cs231n4-0.png)

다음과 같이 input이 x, W인 선형 classifier가 있다.

곱셈 노드는 행렬 곱셈을 나타낸다. 파라미터 W와 데이터 x의 곱셈은 socre vector를 출력하게 된다. 그리고 hinge loss라는 노드와 regularization 항 또한 가지고 있다.

노드 R은 regularizaiton 항을 계산하고, 최종 loss인 L은 regularization 항과 데이터 항의 합이 된다.

computational graph를 사용해서 함수를 표현하게 됨으로써 backpropagation이라고 부르는 기술을 사용할 수 있게 된다.

backpropagation은 gradient를 얻기 위해 computational graph 내부의 모든 변수에 대해 chain rule을 재귀적으로 사용한다.

backpropagation의 간단한 예제를 살펴보자.

![](/img/cs231n4-1.png)

함수 f와 f를 computational graph로 나타낸 것이 있다.

![](/img/images_fbdp1202_post_127691cc-4c99-4d8c-8b88-1e0fea68eb65_cs231n-04-002-Backpropagation_example_1.png)

backpropagation을 하려면 마지막부터 처음으로 돌아오면서 진행한다.

chain rule을 사용하면 간단하게 backpropagation을 할 수 있다.  

각각의 gradient를 구해보자.

![](/img/cs231n4image.png)

![](/img/images_fbdp1202_post_c9ded85e-5a7b-43f5-95b9-cdba33d64e5c_cs231n-04-007-Backpropagation_fig.png)

우리는 backpropagation을 통해 각각의 local gradient를 얻고, 이 값들은 상위 방향으로 계속해서 전달된다.

![](/img/images_fbdp1202_post_ced99e91-1448-43de-963c-a5d359a44ab1_cs231n-04-008-Backpropagation_another_example_1.png)

다음 예시를 보자.

최종 변수에 대한 출력의 gradient는 1이다. 그렇다면 이전 단계로 넘어가보자.

1/x의 input에 대한 gradient, 즉 upstream gradient가 1이고, 밑에 유도식을 보면 1/x을 local gradient인 x에 대해서 미분하면 -1/x^2이 된다. 그리고 이 둘을 곱하면 끝.

x = 1.37이고 upstream gradient는 1이므로 -1/1.37^2 * 1 = -0.53이 된다!

그리고 이 -0.53은 이전 스텝의 upstream gradient가 될 것이고, 이를 반복하면 된다.

또한 곱셈 노드에서는 input에 대한 local gradient는 다른 input의 값이라는 것과, 덧셈 노드에서는 input에 대한 local gradient가 1이라는 것도 위 예제에서 알 수 있었다.

![](/img/images_fbdp1202_post_43e2d844-8874-4c9e-bb4c-595172a5f318_cs231n-04-009-Backpropagation_another_example_2.png)

위 사진과 같이 그 노드에 대한 local gradient를 적어두기만 하면 노드들을 더 복잡한 그룹으로 묶을 수도 있다.

![](/img/images_fbdp1202_post_32435542-2732-4fbd-a550-9fd4498e7b8b_cs231n-04-012-Gradients_for_vectorized_code.png)

만약 input이 스칼라가 아닌 벡터라면 다변수 벡터 함수에 대한 일차미분값을 의미하는 Jacobian matrix를 사용하게 된다.

만약 4096-d input vector가 들어오게 된다면 Jacobian matrix의 크기는 4096x4096이 될 것이다. 여기에다가 minibatch가 100이라면 409600x409600으로 굉장히 커질 것이다. 

다음 예시를 살펴보자.

![](/img/images_fbdp1202_post_ada59bf8-2c42-4e1f-9f66-e507095cdb29_cs231n-04-014-vectorized_operationis_examples_1.png)

![](/img/images_fbdp1202_post_79735649-a775-46cf-a01c-ea5bfaea9bea_cs231n-04-016-vectorized_operationis_examples_3.png)

![](/img/cs231n4image1.png)

![](/img/cs231n4image2.png)

![](/img/cs231n4image3.png)

+ 변수와 gradient는 항상 같은 shape을 가진다. 그리고 gradient의 요소는 최종 출력에 얼마나 영향을 미치는지를 정량화 한다.

요약하자면, 

gradient를 구하기 위해서 backpropagation을 사용한다. 

Backpropagation은 computational graph에서 chain rule을 재귀적으로 적용한 것.

forward pass에서 우리는 연산 결과를 계산하고 저장하는데, 이것은 나중에 gradient를 계산할 때 backward pass에서 chain rule을 사용하기 위함이고, upstream gradient와 저장한 값들을 곱해 각 노드의 input에 대한 gradient를 구하고 이전 노드로 통과시킨다…

### Neural Networks

![](/img/cs231n4-2.png)

위 사진과 같이 함수를 계층적으로 쌓을 수 있다. 신경망은 함수들의 집합이라고 할 수 있는데, 비선형의 복잡한 함수를 만들기 위해서 간단한 함수들을 계층적으로 쌓아 올리는 것이다.